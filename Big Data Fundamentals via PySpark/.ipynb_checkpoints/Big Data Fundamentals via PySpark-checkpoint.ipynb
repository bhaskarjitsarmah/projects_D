{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Processing Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>**Hadoop/MapReduce**: Scalable and fault tolerant framework written in Java\n",
    "    <li>Open Source</li>\n",
    "    <li>Used a Batch Processing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>**Apache Spark**: General purpose and lightning fast cluster computing system\n",
    "    <li>Open Source</li>\n",
    "    <li>Used as Batch Processing and Real Time Data Processing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Apache Spark Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Distributed cluster computing framework</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](apache spark components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark mode of deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>**Local mode**: It is a single machine, such as the laptop. It is convenient for testing, debugging and demonstration</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>**Cluster mode**: Set of pre-defined machines, good for production</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Interactive environment for running Spark jobs</li>\n",
    "<li>Helpful for fast interactive prototyping</li>\n",
    "<li>Spark's shell allows interacting with data on disk or in memory</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>Three different Spark shells - \n",
    "    <li>Spark-shell for Scala</li>\n",
    "    <li>PySpark-shell for Python</li>\n",
    "    <li>SparkR for R</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkContext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 2.4.0\n",
      "The Python version of Spark Context in the PySpark shell is 3.6\n",
      "The master of Spark Context in the PySpark shell is local[*]\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive use of PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(1, 100)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data in PySpark Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile('Complete_Shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext's `textFile` method is quite powerful for creating distributed collections of unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Programming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of lambda with map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of lambda with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>**Resilient**: ability withstand failure</li>\n",
    "<li>**Distributed**: Spanning across multiple machines</li>\n",
    "<li>**Datasets**: Collection of partitioned data e.g - Arrays, Tables, Tuples etc</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create RDDs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Parallelizing an existing collection of objects using **sc.parallelize()**</li>\n",
    "    <li>External Datasets: \n",
    "        <ol>Files in HDFS</ol>\n",
    "        <ol>Objects in Amazon S3 bucket</ol>\n",
    "        <ol>lines in a text file</ol>\n",
    "    </li>\n",
    "    <li>From existing RDDs</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from parallelized collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from external datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(\"Complete_Shakespeare.txt\")\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n",
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(\"Complete_Shakespeare.txt\", minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD operations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs in PySpark supports two types of operations - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>**Transformations**: Transformations create new RDDs e.g. map(), filter(), flatMap(), unions()</li>\n",
    "<li>**Actions**: Actions perform computation on new RDDs e.g. collect(), take(), first(), count()</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = range(1, 10)\n",
    "numbRDD = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDDs in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>**reduce**: reduce(func) action is used for aggregating the elements of a regular RDD</li>\n",
    "<li>**saveAsTextFile**: saveAsTextFile() action saves RDD into a text file inside a directory with each partition as a separate file</li>\n",
    "<li>**coalesce**: coalesce() method can be used to save RDD in a single text file</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD and Transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(\"Complete_Shakespeare.txt\")\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and reducing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples\n",
    "sample_list = [('Mona', 20), ('Jennifer', 34), ('John', 20), ('Jim', 26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CSV into DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of people_df is  <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is \", type(people_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Data with PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+------+-------------+\n",
      "|_c0|person_id|            name|   sex|date of birth|\n",
      "+---+---------+----------------+------+-------------+\n",
      "|  0|      100|  Penelope Lewis|female|   1990-08-31|\n",
      "|  1|      101|   David Anthony|  male|   1971-10-14|\n",
      "|  2|      102|       Ida Shipp|female|   1962-05-24|\n",
      "|  3|      103|    Joanna Moore|female|   2017-03-10|\n",
      "|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|  5|      105|   David Simmons|  male|   1999-12-30|\n",
      "|  6|      106|   Edward Hudson|  male|   1983-05-09|\n",
      "|  7|      107|    Albert Jones|  male|   1990-09-13|\n",
      "|  8|      108|Leonard Cavender|  male|   1958-08-08|\n",
      "|  9|      109|  Everett Vadala|  male|   2005-05-24|\n",
      "+---+---------+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There are 100000 rows in the people_df DataFrame.\n",
      "There are 5 columns in the people_df DataFrame and their names are ['_c0', 'person_id', 'name', 'sex', 'date of birth']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark DataFrames subsetting and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------+\n",
      "|            name|   sex|date of birth|\n",
      "+----------------+------+-------------+\n",
      "|  Penelope Lewis|female|   1990-08-31|\n",
      "|   David Anthony|  male|   1971-10-14|\n",
      "|       Ida Shipp|female|   1962-05-24|\n",
      "|    Joanna Moore|female|   2017-03-10|\n",
      "|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|   David Simmons|  male|   1999-12-30|\n",
      "|   Edward Hudson|  male|   1983-05-09|\n",
      "|    Albert Jones|  male|   1990-09-13|\n",
      "|Leonard Cavender|  male|   1958-08-08|\n",
      "|  Everett Vadala|  male|   2005-05-24|\n",
      "+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There were 100000 rows before removing duplicates, and 99998 rows after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), \n",
    "                                                                                                    people_df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_df_female DataFrame and 49066 rows in the people_df_male DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), \n",
    "                                                                                                               people_df_male.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            name|\n",
      "+----------------+\n",
      "|  Penelope Lewis|\n",
      "|   David Anthony|\n",
      "|       Ida Shipp|\n",
      "|    Joanna Moore|\n",
      "|  Lisandra Ortiz|\n",
      "|   David Simmons|\n",
      "|   Edward Hudson|\n",
      "|    Albert Jones|\n",
      "|Leonard Cavender|\n",
      "|  Everett Vadala|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a \"query\" to select the names of the people\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL queries on filtering table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_female_df and 49066 rows in the people_male_df DataFrames\n"
     ]
    }
   ],
   "source": [
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), \n",
    "                                                                                                      people_male_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark DataFrame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [(\"Mona\", 20), (\"Jennifer\", 34), (\"John\", 20), (\"Jim\", 26)]\n",
    "names_rdd = sc.parallelize(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = spark.createDataFrame(names_rdd, schema=['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on FIFA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Photo: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      " |-- Overall: integer (nullable = true)\n",
      " |-- Potential: integer (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- Club Logo: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Wage: string (nullable = true)\n",
      " |-- Special: integer (nullable = true)\n",
      " |-- Acceleration: string (nullable = true)\n",
      " |-- Aggression: string (nullable = true)\n",
      " |-- Agility: string (nullable = true)\n",
      " |-- Balance: string (nullable = true)\n",
      " |-- Ball control: string (nullable = true)\n",
      " |-- Composure: string (nullable = true)\n",
      " |-- Crossing: string (nullable = true)\n",
      " |-- Curve: string (nullable = true)\n",
      " |-- Dribbling: string (nullable = true)\n",
      " |-- Finishing: string (nullable = true)\n",
      " |-- Free kick accuracy: string (nullable = true)\n",
      " |-- GK diving: string (nullable = true)\n",
      " |-- GK handling: string (nullable = true)\n",
      " |-- GK kicking: string (nullable = true)\n",
      " |-- GK positioning: string (nullable = true)\n",
      " |-- GK reflexes: string (nullable = true)\n",
      " |-- Heading accuracy: string (nullable = true)\n",
      " |-- Interceptions: string (nullable = true)\n",
      " |-- Jumping: string (nullable = true)\n",
      " |-- Long passing: string (nullable = true)\n",
      " |-- Long shots: string (nullable = true)\n",
      " |-- Marking: string (nullable = true)\n",
      " |-- Penalties: string (nullable = true)\n",
      " |-- Positioning: string (nullable = true)\n",
      " |-- Reactions: string (nullable = true)\n",
      " |-- Short passing: string (nullable = true)\n",
      " |-- Shot power: string (nullable = true)\n",
      " |-- Sliding tackle: string (nullable = true)\n",
      " |-- Sprint speed: string (nullable = true)\n",
      " |-- Stamina: string (nullable = true)\n",
      " |-- Standing tackle: string (nullable = true)\n",
      " |-- Strength: string (nullable = true)\n",
      " |-- Vision: string (nullable = true)\n",
      " |-- Volleys: string (nullable = true)\n",
      " |-- CAM: double (nullable = true)\n",
      " |-- CB: double (nullable = true)\n",
      " |-- CDM: double (nullable = true)\n",
      " |-- CF: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LAM: double (nullable = true)\n",
      " |-- LB: double (nullable = true)\n",
      " |-- LCB: double (nullable = true)\n",
      " |-- LCM: double (nullable = true)\n",
      " |-- LDM: double (nullable = true)\n",
      " |-- LF: double (nullable = true)\n",
      " |-- LM: double (nullable = true)\n",
      " |-- LS: double (nullable = true)\n",
      " |-- LW: double (nullable = true)\n",
      " |-- LWB: double (nullable = true)\n",
      " |-- Preferred Positions: string (nullable = true)\n",
      " |-- RAM: double (nullable = true)\n",
      " |-- RB: double (nullable = true)\n",
      " |-- RCB: double (nullable = true)\n",
      " |-- RCM: double (nullable = true)\n",
      " |-- RDM: double (nullable = true)\n",
      " |-- RF: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- RS: double (nullable = true)\n",
      " |-- RW: double (nullable = true)\n",
      " |-- RWB: double (nullable = true)\n",
      " |-- ST: double (nullable = true)\n",
      "\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|               Club|           Club Logo| Value| Wage|Special|Acceleration|Aggression|Agility|Balance|Ball control|Composure|Crossing|Curve|Dribbling|Finishing|Free kick accuracy|GK diving|GK handling|GK kicking|GK positioning|GK reflexes|Heading accuracy|Interceptions|Jumping|Long passing|Long shots|Marking|Penalties|Positioning|Reactions|Short passing|Shot power|Sliding tackle|Sprint speed|Stamina|Standing tackle|Strength|Vision|Volleys| CAM|  CB| CDM|  CF|  CM|    ID| LAM|  LB| LCB| LCM| LDM|  LF|  LM|  LS|  LW| LWB|Preferred Positions| RAM|  RB| RCB| RCM| RDM|  RF|  RM|  RS|  RW| RWB|  ST|\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|  0|Cristiano Ronaldo| 32|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|     Real Madrid CF|https://cdn.sofif...|€95.5M|€565K|   2228|          89|        63|     89|     63|          93|       95|      85|   81|       91|       94|                76|        7|         11|        15|            14|         11|              88|           29|     95|          77|        92|     22|       85|         95|       96|           83|        94|            23|          91|     92|             31|      80|    85|     88|89.0|53.0|62.0|91.0|82.0| 20801|89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|             ST LW |89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|92.0|\n",
      "|  1|         L. Messi| 30|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     93|       93|       FC Barcelona|https://cdn.sofif...| €105M|€565K|   2154|          92|        48|     90|     95|          95|       96|      77|   89|       97|       95|                90|        6|         11|        15|            14|          8|              71|           22|     68|          87|        88|     13|       74|         93|       95|           88|        85|            26|          87|     73|             28|      59|    90|     85|92.0|45.0|59.0|92.0|84.0|158023|92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|                RW |92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|88.0|\n",
      "|  2|           Neymar| 25|https://cdn.sofif...|     Brazil|https://cdn.sofif...|     92|       94|Paris Saint-Germain|https://cdn.sofif...| €123M|€280K|   2100|          94|        56|     96|     82|          95|       92|      75|   81|       96|       89|                84|        9|          9|        15|            15|         11|              62|           36|     61|          75|        77|     21|       81|         90|       88|           81|        80|            33|          90|     78|             24|      53|    80|     83|88.0|46.0|59.0|88.0|79.0|190871|88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|                LW |88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|84.0|\n",
      "|  3|        L. Suárez| 30|https://cdn.sofif...|    Uruguay|https://cdn.sofif...|     92|       92|       FC Barcelona|https://cdn.sofif...|  €97M|€510K|   2291|          88|        78|     86|     60|          91|       83|      77|   86|       86|       94|                84|       27|         25|        31|            33|         37|              77|           41|     69|          64|        86|     30|       85|         92|       93|           83|        87|            38|          77|     89|             45|      80|    84|     88|87.0|58.0|65.0|88.0|80.0|176580|87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|                ST |87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|88.0|\n",
      "|  4|         M. Neuer| 31|https://cdn.sofif...|    Germany|https://cdn.sofif...|     92|       92|   FC Bayern Munich|https://cdn.sofif...|  €61M|€230K|   1493|          58|        29|     52|     35|          48|       70|      15|   14|       30|       13|                11|       91|         90|        95|            91|         89|              25|           30|     78|          59|        16|     10|       47|         12|       85|           55|        25|            11|          61|     44|             10|      83|    70|     11|null|null|null|null|null|167495|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  5|   R. Lewandowski| 28|https://cdn.sofif...|     Poland|https://cdn.sofif...|     91|       91|   FC Bayern Munich|https://cdn.sofif...|  €92M|€355K|   2143|          79|        80|     78|     80|          89|       87|      62|   77|       85|       91|                84|       15|          6|        12|             8|         10|              85|           39|     84|          65|        83|     25|       81|         91|       91|           83|        88|            19|          83|     79|             42|      84|    78|     87|84.0|57.0|62.0|87.0|78.0|188545|84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|                ST |84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|88.0|\n",
      "|  6|           De Gea| 26|https://cdn.sofif...|      Spain|https://cdn.sofif...|     90|       92|  Manchester United|https://cdn.sofif...|€64.5M|€215K|   1458|          57|        38|     60|     43|          42|       64|      17|   21|       18|       13|                19|       90|         85|        87|            86|         90|              21|           30|     67|          51|        12|     13|       40|         12|       88|           50|        31|            13|          58|     40|             21|      64|    68|     13|null|null|null|null|null|193080|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  7|        E. Hazard| 26|https://cdn.sofif...|    Belgium|https://cdn.sofif...|     90|       91|            Chelsea|https://cdn.sofif...|€90.5M|€295K|   2096|          93|        54|     93|     91|          92|       87|      80|   82|       93|       83|                79|       11|         12|         6|             8|          8|              57|           41|     59|          81|        82|     25|       86|         85|       85|           86|        79|            22|          87|     79|             27|      65|    86|     79|88.0|47.0|61.0|87.0|81.0|183277|88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|                LW |88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|82.0|\n",
      "|  8|         T. Kroos| 27|https://cdn.sofif...|    Germany|https://cdn.sofif...|     90|       90|     Real Madrid CF|https://cdn.sofif...|  €79M|€340K|   2165|          60|        60|     71|     69|          89|       85|      85|   85|       79|       76|                84|       10|         11|        13|             7|         10|              54|           85|     32|          93|        90|     63|       73|         79|       86|           90|        87|            69|          52|     77|             82|      74|    88|     82|83.0|72.0|82.0|81.0|87.0|182521|83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|            CDM CM |83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|77.0|\n",
      "|  9|       G. Higuaín| 29|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     90|       90|           Juventus|https://cdn.sofif...|  €77M|€275K|   1961|          78|        50|     75|     69|          85|       86|      68|   74|       84|       91|                62|        5|         12|         7|             5|         10|              86|           20|     79|          59|        82|     12|       70|         92|       88|           75|        88|            18|          80|     72|             22|      85|    70|     88|81.0|46.0|52.0|84.0|71.0|167664|81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|                ST |81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|87.0|\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17981 rows in the fifa_df DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(\"Fifa2018_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              Age|\n",
      "+-------+-----------------+\n",
      "|  count|             1140|\n",
      "|   mean|24.20263157894737|\n",
      "| stddev|4.197096712293752|\n",
      "|    min|               16|\n",
      "|    max|               36|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\" \n",
    "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic stastics\n",
    "fifa_df_germany_age.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "# Plot the 'Age' density of Germany Players\n",
    "fifa_df_germany_age_pandas['Age'].plot(kind='density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark MLlib can only support RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into RDD\n",
    "data = sc.textFile(\"ratings.csv\")\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 243, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c3711116acc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the ALS model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Drop the ratings column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtestdata_no_rating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\mllib\\recommendation.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)\u001b[0m\n\u001b[0;32m    271\u001b[0m           \u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[1;32m--> 273\u001b[1;33m         model = callMLlibFunc(\"trainALSModel\", cls._prepare(ratings), rank, iterations,\n\u001b[0m\u001b[0;32m    274\u001b[0m                               lambda_, blocks, nonnegative, seed)\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\mllib\\recommendation.py\u001b[0m in \u001b[0;36m_prepare\u001b[1;34m(cls, ratings)\u001b[0m\n\u001b[0;32m    228\u001b[0m             raise TypeError(\"Ratings should be represented by either an RDD or a DataFrame, \"\n\u001b[0;32m    229\u001b[0m                             \"but got %s.\" % type(ratings))\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \"\"\"\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 243, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
